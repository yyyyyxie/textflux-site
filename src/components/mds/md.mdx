<h1 style={{ textAlign: 'left' }}>Model Architecture</h1>



<figure>
  <img src="./paper/arch_v3_2.png" alt="Architecture of TextFlux" style={{ maxWidth: '100%'}} />
  <figcaption style={{ fontSize: '0.95em', color: '#666', marginTop: '0.5em' }}>
    We propose an OCR-free scene text synthesis method that spatially concatenates glyph-rendered text with the original image as model input, enabling the diffusion transformer to leverage its inherent context-awareness to render text in the masked regions.
  </figcaption>
</figure>


<h3 style={{ textAlign: 'left' }}>Key Features</h3>


* **OCR-Free:** Simplified architecture without OCR encoders.
* **High-Fidelity & Contextual Styles:** Precise rendering, stylistically consistent with scenes.
* **Multilingual & Low-Resource:** Strong performance across languages, adapts to new languages with minimal data.
* **Zero-Shot Generalization:** Renders characters unseen during training.
* **Controllable Multi-Line Text:** Flexible multi-line synthesis with line-level control.
* **Data Efficient:** Uses a fraction of data (e.g., ~1%) compared to other methods.



<h1 style={{ textAlign: 'left' }}>Experiment Results</h1>

<figure>
  <img src="./paper/Table1.png" alt="Architecture of TextFlux" style={{ maxWidth: '100%'}} />
  <figcaption style={{ fontSize: '0.95em', color: '#666', marginTop: '-1.0em' }}>
    Quantitative comparison of multi-line text generation metrics against baselines.
    We use Sequence Accuracy (SeqAcc) as the main evaluation metric to measure recognition correctness. 
    The best scores are highlighted in bold. The second-best results are underlined. FID and LPIPS are computed on the ReCTS dataset. 
    The User Study (US) is conducted to capture human evaluations regarding the overall quality of generated images (score range: 0--10).
  </figcaption>
</figure>


<h1 style={{ textAlign: 'left' }}>Ablation Study</h1>

<figure>
  <img src="./paper/Table2.png" alt="Architecture of TextFlux" style={{ maxWidth: '100%' }} />
  <figcaption style={{ fontSize: '0.95em', color: '#666', marginTop: '-1.0em' }}>
    <strong>First: </strong>SeqAcc-Recon results on the ReCTS and TotalText datasets using different training strategies.
    <strong>Second: </strong>Evaluating different text encoders based on SeqAcc-Recon results when provided with empty input prompts.
  </figcaption>

</figure>


<h1 style={{ textAlign: 'left' }}>Visual Comparison Results</h1>

<figure>
  <img src="./paper/abstract_fig_2 2.png" alt="Architecture of TextFlux" style={{ maxWidth: '100%' }} />
  <figcaption style={{ fontSize: '0.95em', color: '#666', marginTop: '-1.0em' }}>
    Some examples of high-fidelity multilingual scene text images generated by our TextFlux.
  </figcaption>

</figure>
